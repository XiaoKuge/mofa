# 1. vllm部署出现问题
vllm serve /Users/chenzi/project/zcbc/mofa/python/agent-hub/qwen-model/Qwen/Qwen3-8B \
--host 127.0.0.1 \
--port 8000 \
--max-model-len 2048 \
--enable-auto-tool-choice \
--tool-call-parser hermes
INFO 07-30 11:11:57 [__init__.py:235] Automatically detected platform cpu.
WARNING 07-30 11:11:58 [_custom_ops.py:20] Failed to import from vllm._C with ImportError("dlopen(/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/_C.abi3.so, 0x0002): symbol not found in flat namespace '__Z14int8_scaled_mmRN2at6TensorERKS0_S3_S3_S3_RKNSt3__18optionalIS0_EE'")
INFO 07-30 11:11:59 [api_server.py:1755] vLLM API server version 0.10.0
INFO 07-30 11:11:59 [cli_args.py:261] non-default args: {'model_tag': '/Users/chenzi/project/zcbc/mofa/python/agent-hub/qwen-model/Qwen/Qwen3-8B', 'host': '127.0.0.1', 'enable_auto_tool_choice': True, 'tool_call_parser': 'hermes', 'model': '/Users/chenzi/project/zcbc/mofa/python/agent-hub/qwen-model/Qwen/Qwen3-8B', 'max_model_len': 2048}
WARNING 07-30 11:12:03 [config.py:3392] Your device 'cpu' doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 07-30 11:12:03 [config.py:3443] Casting torch.bfloat16 to torch.float16.
INFO 07-30 11:12:03 [config.py:1604] Using max model len 2048
INFO 07-30 11:12:03 [arg_utils.py:1030] Chunked prefill is not supported for ARM and POWER CPUs; disabling it for V1 backend.
INFO 07-30 11:12:06 [__init__.py:235] Automatically detected platform cpu.
WARNING 07-30 11:12:07 [_custom_ops.py:20] Failed to import from vllm._C with ImportError("dlopen(/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/_C.abi3.so, 0x0002): symbol not found in flat namespace '__Z14int8_scaled_mmRN2at6TensorERKS0_S3_S3_S3_RKNSt3__18optionalIS0_EE'")
INFO 07-30 11:12:08 [core.py:572] Waiting for init message from front-end.
INFO 07-30 11:12:08 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/Users/chenzi/project/zcbc/mofa/python/agent-hub/qwen-model/Qwen/Qwen3-8B', speculative_config=None, tokenizer='/Users/chenzi/project/zcbc/mofa/python/agent-hub/qwen-model/Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/Users/chenzi/project/zcbc/mofa/python/agent-hub/qwen-model/Qwen/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"level":2,"debug_dump_path":"","cache_dir":"","backend":"inductor","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false,"dce":true,"size_asserts":false,"nan_asserts":false,"epilogue_fusion":true},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}
INFO 07-30 11:12:08 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
ERROR 07-30 11:12:08 [core.py:632] EngineCore failed to start.
ERROR 07-30 11:12:08 [core.py:632] Traceback (most recent call last):
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 623, in run_engine_core
ERROR 07-30 11:12:08 [core.py:632]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 441, in __init__
ERROR 07-30 11:12:08 [core.py:632]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 77, in __init__
ERROR 07-30 11:12:08 [core.py:632]     self.model_executor = executor_class(vllm_config)
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 53, in __init__
ERROR 07-30 11:12:08 [core.py:632]     self._init_executor()
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 48, in _init_executor
ERROR 07-30 11:12:08 [core.py:632]     self.collective_rpc("init_device")
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
ERROR 07-30 11:12:08 [core.py:632]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/utils/__init__.py", line 2985, in run_method
ERROR 07-30 11:12:08 [core.py:632]     return func(*args, **kwargs)
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 603, in init_device
ERROR 07-30 11:12:08 [core.py:632]     self.worker.init_device()  # type: ignore
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/worker/cpu_worker.py", line 60, in init_device
ERROR 07-30 11:12:08 [core.py:632]     ret = torch.ops._C_utils.init_cpu_threads_env(self.local_omp_cpuid)
ERROR 07-30 11:12:08 [core.py:632]   File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_ops.py", line 1267, in __getattr__
ERROR 07-30 11:12:08 [core.py:632]     raise AttributeError(
ERROR 07-30 11:12:08 [core.py:632] AttributeError: '_OpNamespace' '_C_utils' object has no attribute 'init_cpu_threads_env'
Process EngineCore_0:
Traceback (most recent call last):
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 636, in run_engine_core
    raise e
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 623, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 441, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 77, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 53, in __init__
    self._init_executor()
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 48, in _init_executor
    self.collective_rpc("init_device")
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/utils/__init__.py", line 2985, in run_method
    return func(*args, **kwargs)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 603, in init_device
    self.worker.init_device()  # type: ignore
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/worker/cpu_worker.py", line 60, in init_device
    ret = torch.ops._C_utils.init_cpu_threads_env(self.local_omp_cpuid)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_ops.py", line 1267, in __getattr__
    raise AttributeError(
AttributeError: '_OpNamespace' '_C_utils' object has no attribute 'init_cpu_threads_env'
Traceback (most recent call last):
  File "/Users/chenzi/env/miniconda3/envs/py310/bin/vllm", line 8, in <module>
    sys.exit(main())
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/entrypoints/cli/main.py", line 54, in main
    args.dispatch_function(args)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/entrypoints/cli/serve.py", line 52, in cmd
    uvloop.run(run_server(args))
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/uvloop/__init__.py", line 82, in run
    return loop.run_until_complete(wrapper())
  File "uvloop/loop.pyx", line 1517, in uvloop.loop.Loop.run_until_complete
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1791, in run_server
    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1811, in run_server_worker
    async with build_async_engine_client(args, client_config) as engine_client:
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/contextlib.py", line 199, in __aenter__
    return await anext(self.gen)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/contextlib.py", line 199, in __aenter__
    return await anext(self.gen)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 194, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 163, in from_vllm_config
    return cls(
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 117, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 98, in make_async_mp_client
    return AsyncMPClient(*client_args)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 677, in __init__
    super().__init__(
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 408, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 697, in launch_core_engines
    wait_for_engine_startup(
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 750, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_0': 1}


# 2. ollama部署qweb3:8b的问题
ollama create qwen3-8b-max -f Modelfile
copying file sha256:b5ee7de71fbf17db3d5704e0c8f2bc7d005ca9e1d7ca2aeb19827b0cfcaa917a 100% 
copying file sha256:2325da0f15bb848e018c5ae071b7943332e9f871d6b60e2ed22ca97d4cb993d2 100% 
copying file sha256:ca10d7e9fb3ed18575dd1e277a2579c16d108e32f27439684afa0e10b1440910 100% 
copying file sha256:c5185c4794be2d8a9784d5753c9922db38df478ce11f9ed0b415b7304d896836 100% 
copying file sha256:f7c4eadfbbf522470667b797a3c89be2524832d2d599797248dc304fff447c30 100% 
copying file sha256:d5d09f07b48c3086c508b30d1c9114bd1189145b74e982a265350c923acd8101 100% 
copying file sha256:f9fdbcb91c23971c13ec5d5f2573d2349e8f61f2f049371ec699281748fdb1bc 100% 
copying file sha256:5991236cea6fe21f3d43cab0f0e84448734fbbe0789816202989f2ddc9d18282 100% 
copying file sha256:aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4 100% 
copying file sha256:11976904f91dfbedc8737fa2dbf3a359cef5c9aa0e5ee48a6db4874e30d4a144 100% 
copying file sha256:31d6a825ae35f11fb85b195b4c42c146c051e446433125a215336abdf95cbf5f 100% 
converting model 
Error: unsupported architecture "Qwen3ForCausalLM"


# 3. ollama部署qwen3:8b-gguf不支持tool
/Users/chenzi/env/miniconda3/envs/py310/bin/python -X pycache_prefix=/Users/chenzi/Library/Caches/JetBrains/PyCharm2025.1/cpython-cache /Users/chenzi/Applications/PyCharm 2025.1.3.1.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 64785 --file /Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp/qwen_controller_ollama_gguf.py 
已连接到 pydev 调试器(内部版本号 251.26927.90)[计时] 初始化 LLM 耗时： 0.093 秒
[计时] 创建 MCP 客户端对象耗时： 0.003 秒
Exception ignored in: <async_generator object HTTP11ConnectionByteStream.__aiter__ at 0x120a4c5c0>
Traceback (most recent call last):
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/httpcore/_async/connection_pool.py", line 364, in __aiter__
    yield part
RuntimeError: async generator ignored GeneratorExit
Exception ignored in: <coroutine object HTTP11ConnectionByteStream.aclose at 0x120a38740>
Traceback (most recent call last):
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/httpcore/_async/http11.py", line 355, in aclose
    await self._connection._response_closed()
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/httpcore/_async/http11.py", line 246, in _response_closed
    async with self._state_lock:
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/httpcore/_synchronization.py", line 76, in __aenter__
    await self._anyio_lock.acquire()
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 1799, in acquire
    await AsyncIOBackend.cancel_shielded_checkpoint()
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 2349, in cancel_shielded_checkpoint
    with CancelScope(shield=True):
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 457, in __exit__
    raise RuntimeError(
RuntimeError: Attempted to exit cancel scope in a different task than it was entered in
Exception ignored in: <async_generator object HTTP11ConnectionByteStream.__aiter__ at 0x120a4cfc0>
Traceback (most recent call last):
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/httpcore/_async/connection_pool.py", line 364, in __aiter__
    yield part
RuntimeError: async generator ignored GeneratorExit
Exception ignored in: <coroutine object HTTP11ConnectionByteStream.aclose at 0x120a39700>
Traceback (most recent call last):
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/httpcore/_async/http11.py", line 355, in aclose
    await self._connection._response_closed()
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/httpcore/_async/http11.py", line 246, in _response_closed
    async with self._state_lock:
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/httpcore/_synchronization.py", line 76, in __aenter__
    await self._anyio_lock.acquire()
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 1799, in acquire
    await AsyncIOBackend.cancel_shielded_checkpoint()
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 2349, in cancel_shielded_checkpoint
    with CancelScope(shield=True):
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 457, in __exit__
    raise RuntimeError(
RuntimeError: Attempted to exit cancel scope in a different task than it was entered in
[计时] 获取工具列表耗时： 0.008 秒

--- 从 FastMCP 服务器发现的原始工具信息 ---
工具 1:
 名称: deepsearch
 描述: 深度搜索工具，基于 serper + LLM 进行搜索和总结的深度思考工具
 输入 Schema: {'properties': {'query': {'title': 'Query', 'type': 'string'}, 'top_k': {'default': 5, 'title': 'Top K', 'type': 'integer'}}, 'required': ['query'], 'type': 'object'}
工具 2:
 名称: generate_mofa_node
 描述: 生成 MoFA 框架中用于构造节点（MoFA Node）的配置或代码片段。向 LLM 传入用户输入脚本、需求或描述，由 OpenAI 模型生成符合 MoFA DataFlow 或 Node 定义规范的内容。
 输入 Schema: {'properties': {'user_input': {'title': 'User Input', 'type': 'string'}}, 'required': ['user_input'], 'type': 'object'}
--------------------------------------------------

[计时] 构建工具定义耗时： 0.0 秒

--- 格式化后发送给 OpenAI API 的工具定义 ---
[
  {
    "type": "function",
    "function": {
      "name": "deepsearch",
      "description": "深度搜索工具，基于 serper + LLM 进行搜索和总结的深度思考工具",
      "parameters": {
        "properties": {
          "query": {
            "title": "Query",
            "type": "string"
          },
          "top_k": {
            "default": 5,
            "title": "Top K",
            "type": "integer"
          }
        },
        "required": [
          "query"
        ],
        "type": "object"
      }
    }
  },
  {
    "type": "function",
    "function": {
      "name": "generate_mofa_node",
      "description": "生成 MoFA 框架中用于构造节点（MoFA Node）的配置或代码片段。向 LLM 传入用户输入脚本、需求或描述，由 OpenAI 模型生成符合 MoFA DataFlow 或 Node 定义规范的内容。",
      "parameters": {
        "properties": {
          "user_input": {
            "title": "User Input",
            "type": "string"
          }
        },
        "required": [
          "user_input"
        ],
        "type": "object"
      }
    }
  }
]
----------------------------------------------------

Traceback (most recent call last):
  File "/Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp/qwen_controller_ollama_gguf.py", line 95, in main
    plan_resp = planner_llm.chat.completions.create(
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", line 925, in create
    return self._post(
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/openai/_base_client.py", line 1249, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'registry.ollama.ai/library/qwen3-8b-local:latest does not support tools', 'type': 'api_error', 'param': None, 'code': None}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/chenzi/Applications/PyCharm 2025.1.3.1.app/Contents/plugins/python/helpers-pro/pydevd_asyncio/pydevd_nest_asyncio.py", line 138, in run
    return loop.run_until_complete(task)
  File "/Users/chenzi/Applications/PyCharm 2025.1.3.1.app/Contents/plugins/python/helpers-pro/pydevd_asyncio/pydevd_nest_asyncio.py", line 243, in run_until_complete
    return f.result()
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/asyncio/futures.py", line 201, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/Users/chenzi/env/miniconda3/envs/py310/lib/python3.10/asyncio/tasks.py", line 232, in __step
    result = coro.send(None)
  File "/Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp/qwen_controller_ollama_gguf.py", line 28, in main
    async with client:
TypeError: 'NoneType' object is not callable
python-BaseException

进程已结束，退出代码为 1


# 4. ollama部署qwen3:8b-gguf存在自言自语 乱说 然后陷入死循环的问题

# 5. llama.cpp部署qwen3:8b-gguf

./bin/llama-server -m "/Users/chenzi/project/zcbc/mofa/python/agent-hub/qwen-model/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf" -c 32768 --port 8080
消耗内存在10g左右 


# 6. ollama部署qwen3:8b时间
分别是 11秒 8秒 5秒等 

# 7. llama.cpp部署qwen3:8b-gguf时间
[计时] LLM 工具选择耗时： 13.353 秒
[计时] 解析工具参数耗时： 0.0 秒

7.814 秒
12.184 秒


# 8. qwen:8b转gguf然后使用llama.cpp部署
cd build && ./bin/llama-server -m "/Users/chenzi/project/zcbc/mofa/python/agent-hub/qwen-model/Qwen/Qwen3-8B-converted/Qwen3-8B-f16.gguf" -c 32768 --port 8080 --jinja -np 4 -cb --no-warmup --threads 8
[计时] LLM 工具选择耗时： 38.09 秒
[计时] 解析工具参数耗时： 0.0 秒
15秒


# 9. 测试时间
## 9.1 4个tool->llama.cpp
用户: 当前的时间是多少？
[计时] LLM 思考（工具选择或回复）耗时：5.737 秒

--- LLM 决定调用工具 ---
  工具名称：get_current_time_and_date
  调用参数：{}
[计时] 工具 get_current_time_and_date 调用耗时：0.115 秒

--- 工具调用结果：{'current_time': '2025-07-31 09:54:09'} ---
[计时] 最终回复生成耗时：8.301 秒

Agent: 当前的时间是2025年7月31日09:54:09。

---
用户: 一公里等于多少千米？
[计时] LLM 思考（工具选择或回复）耗时：25.288 秒

Agent: 1公里等于1千米。公里（km）和千米是同一个单位的不同名称，因此两者是等价的。

------
用户: 一斤等于多少盎司
[计时] LLM 思考（工具选择或回复）耗时：20.499 秒

--- LLM 决定调用工具 ---
  工具名称：convert_unit
  调用参数：{'value': 1, 'from_unit': 'jin', 'to_unit': 'oz'}
[计时] 工具 convert_unit 调用耗时：0.322 秒

--- 工具调用结果：{'error': '不支持的单位转换。'} ---
[计时] 最终回复生成耗时：23.612 秒

Agent: 1斤等于17.64盎司（oz）。  
（注：1斤=500克，1盎司≈28.35克，因此500 ÷ 28.35 ≈ 17.64）  

如果需要其他单位转换，可以随时告诉我！


----


# 9. 测试时间
## 9.1 8个tool->llama.cpp
用户: 讲一个笑话
[计时] LLM 思考（工具选择或回复）耗时：3.653 秒

--- LLM 决定调用工具 ---
  工具名称：get_random_joke
  调用参数：{}
[计时] 工具 get_random_joke 调用耗时：0.224 秒

--- 工具调用结果：{'joke': '如果一台电脑在下棋时作弊，那它就是一台骗子机。'} ---
[计时] 最终回复生成耗时：5.174 秒

Agent: 哈哈，这里有个笑话：如果一台电脑在下棋时作弊，那它就是一台骗子机！你是不是也被逗笑了？😄

---
用户: 检查 https://mofa.ai/能否访问 
[计时] LLM 思考（工具选择或回复）耗时：5.615 秒

--- LLM 决定调用工具 ---
  工具名称：check_website_status
  调用参数：{'url': 'https://mofa.ai/'}
[计时] 工具 check_website_status 调用耗时：2.259 秒

--- 工具调用结果：{'url': 'https://mofa.ai/', 'status': 'online', 'http_status_code': 200, 'source': 'live_check'} ---
[计时] 最终回复生成耗时：5.929 秒

Agent: 网站 https://mofa.ai/ 当前处于在线状态，HTTP 状态码 200 表示访问正常。您现在可以正常访问该链接。需要我帮您打开链接或了解其他信息吗？

---
用户: 现在几点了？
[计时] LLM 思考（工具选择或回复）耗时：5.923 秒

--- LLM 决定调用工具 ---
  工具名称：get_current_time_and_date
  调用参数：{}
[计时] 工具 get_current_time_and_date 调用耗时：0.13 秒

--- 工具调用结果：{'current_time': '2025-07-31 10:02:46'} ---
[计时] 最终回复生成耗时：10.509 秒

Agent: 当前时间是2025年7月31日 上午10:02:46，您需要我帮您做些什么吗？


-----
## 9.1 16个tool->llama.cpp
用户: 当前文件夹下面有哪些文件
[计时] LLM 思考（工具选择或回复）耗时：27.218 秒

--- LLM 决定调用工具 ---
  工具名称：list_directory_contents
  调用参数：{'directory_path': '.'}
[计时] 工具 list_directory_contents 调用耗时：0.337 秒

--- 工具调用结果：{'directory': '.', 'contents': ['qwen_controller_ollama_gguf.py', '.env.secret', 'more_mcp_server.py', 'qwen_controller_ollama_8b_local.py', 'controller_mcp.py', '.gitignore', 'qwen_controller.py', 'qwen_controller_8b_gguf_llama_cpp.py', 'mcp_server.py'], 'source': 'local_system'} ---
[计时] 最终回复生成耗时：12.302 秒

Agent: 当前文件夹下的文件列表如下：

```
qwen_controller_ollama_gguf.py
.env.secret
more_mcp_server.py
qwen_controller_ollama_8b_local.py
controller_mcp.py
.gitignore
qwen_controller.py
qwen_controller_8b_gguf_llama_cpp.py
mcp_server.py
```

---

用户: ping一下 10.100.1.115
[计时] LLM 思考（工具选择或回复）耗时：23.343 秒

--- LLM 决定调用工具 ---
  工具名称：ping_host
  调用参数：{'host': '10.100.1.115'}
[计时] 工具 ping_host 调用耗时：0.418 秒

--- 工具调用结果：{'host': '10.100.1.115', 'status': '不可达', 'error_message': '模拟的ping失败', 'loss_percent': 100, 'source': 'mock_ping'} ---
[计时] 最终回复生成耗时：32.794 秒

Agent: 主机 10.100.1.115 当前不可达，模拟的 ping 操作失败，丢包率为 100%。这可能由以下原因导致：

1. **网络连接问题**：当前网络环境可能无法访问该 IP 地址  
2. **主机宕机**：目标设备可能已关闭或服务未启动  
3. **防火墙限制**：设备可能配置了阻止 ICMP 请求的规则  

建议您：  
- 确认目标 IP 地址是否正确  
- 检查本地网络连接状态  
- 尝试使用 `telnet` 或 `traceroute` 进行更深入的诊断  
- 联系网络管理员确认设备状态  

是否需要我协助您执行其他网络检测操作？


---
[计时] LLM 思考（工具选择或回复）耗时：13.395 秒

--- LLM 决定调用工具 ---
  工具名称：get_public_ip_address
  调用参数：{}
[计时] 工具 get_public_ip_address 调用耗时：2.133 秒

--- 工具调用结果：{'public_ip': '49.51.200.173', 'source': 'ipify.org'} ---
[计时] 最终回复生成耗时：24.772 秒

Agent: 您的当前公网 IP 地址是 **49.51.200.173**（来源：ipify.org）。此 IP 通常属于阿里云或腾讯云的公网段，如果您在使用云服务器或家庭宽带，请确认网络服务提供商是否正常。需要我帮您进一步检测网络连通性吗？



--------
## 9.1 32个tool->llama.cpp
用户: 获取 /Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp 下面的文件大小 
[计时] LLM 思考（工具选择或回复）耗时：34.8 秒

--- LLM 决定调用工具 ---
  工具名称：get_path_size
  调用参数：{'path': '/Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp'}
[计时] 工具 get_path_size 调用耗时：0.317 秒

--- 工具调用结果：{'path': '/Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp', 'size_bytes': 86256, 'source': 'local_system'} ---
[计时] 最终回复生成耗时：29.795 秒

Agent: 路径 `/Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp` 的总大小为 **86,256 字节**（约 **84.25 KB**）。数据来源于本地系统。是否需要进一步查看目录内具体文件的大小？

---
[计时] LLM 思考（工具选择或回复）耗时：50.318 秒

--- LLM 决定调用工具 ---
  工具名称：run_simple_system_command
  调用参数：{'command': "lsof -i :8000 || echo 'No process found on port 8000'"}
[计时] 工具 run_simple_system_command 调用耗时：0.58 秒

--- 工具调用结果：{'command': "lsof -i :8000 || echo 'No process found on port 8000'", 'output': 'COMMAND     PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME\npython3.1 39638 chenzi   10u  IPv4 0x1fced43de6fced48      0t0  TCP *:irdmi (LISTEN)\n', 'status': 'success', 'source': 'local_system'} ---
[计时] 最终回复生成耗时：45.778 秒

Agent: 当前 **8000端口** 被以下进程占用：

- **进程号 (PID)**: 39638  
- **进程名称**: `python3.1`  
- **占用端口状态**: TCP 监听 (`irdmi` 协议)  
- **用户**: `chenzi`  

是否需要帮助结束该进程？  
（输入 `kill` 终止进程，或 `exit` 暂时跳过）
----

用户: 在 /Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp下面创建一个cc.html的文件
[计时] LLM 思考（工具选择或回复）耗时：63.128 秒

--- LLM 决定调用工具 ---
  工具名称：write_text_to_file
  调用参数：{'file_path': '/Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp/cc.html', 'content': '<!DOCTYPE html>\n<html>\n<head><title>新文件</title></head>\n<body>\n</body>\n</html>'}
[计时] 工具 write_text_to_file 调用耗时：0.325 秒

--- 工具调用结果：{'status': 'success', 'message': "内容已写入文件 '/Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp/cc.html'。", 'source': 'local_system'} ---
[计时] 最终回复生成耗时：32.803 秒

Agent: 文件 `cc.html` 已成功创建在路径 `/Users/chenzi/project/zcbc/mofa/python/examples/dataflow_mcp` 下，内容为一个基础的 HTML 模板。是否需要进一步编辑该文件或查看其内容？


# 10. ollama qwen3:8b运行 no think
[计时] LLM 思考（工具选择或回复）耗时：26.753 秒
--- LLM 决定调用工具 ---
  工具名称：list_directory_contents
  调用参数：{'directory_path': '/'}

用户: 当前的时间是多少？
[计时] LLM 思考（工具选择或回复）耗时：5.051 秒

--- LLM 决定调用工具 ---
  工具名称：get_current_time_and_date
  调用参数：{}
[计时] 工具 get_current_time_and_date 调用耗时：0.116 秒

--- 工具调用结果：{'current_time': '2025-07-31 11:19:54'} ---
[计时] 最终回复生成耗时：0.889 秒


计时] LLM 思考（工具选择或回复）耗时：4.897 秒

--- LLM 决定调用工具 ---
  工具名称：get_public_ip_address
  调用参数：{}
[计时] 工具 get_public_ip_address 调用耗时：1.464 秒

--- 工具调用结果：{'public_ip': '49.51.200.173', 'source': 'ipify.org'} ---
[计时] 最终回复生成耗时：1.476 秒


当前的用户公网 IP 是 **49.51.200.173**，该信息来自服务提供商 **ipify.org**。



[计时] LLM 思考（工具选择或回复）耗时：1.169 秒

--- LLM 决定调用工具 ---
  工具名称：list_directory_contents
  调用参数：{'directory_path': '.'}
[计时] 工具 list_directory_contents 调用耗时：0.315 秒

--- 工具调用结果：{'directory': '.', 'contents': ['qwen_controller_ollama_gguf.py', '.env.secret', 'more_mcp_server.py', 'qwen_controller_ollama_8b_local.py', 'controller_mcp.py', 'cc.html', '.gitignore', 'qwen_controller.py', 'qwen_controller_8b_gguf_llama_cpp.py', 'mcp_server.py'], 'source': 'local_system'} ---
[计时] 最终回复生成耗时：4.148 秒



# 10. ollama qwen3:8b运行 think

用户: 当前的8000端口被占用了吗？
[计时] LLM 思考（工具选择或回复）耗时：20.465 秒

--- LLM 决定调用工具 ---
  工具名称：run_simple_system_command
  调用参数：{'command': "lsof -i :8000 || echo 'Port 8000 is free'"}
[计时] 工具 run_simple_system_command 调用耗时：0.581 秒

--- 工具调用结果：{'command': "lsof -i :8000 || echo 'Port 8000 is free'", 'output': 'COMMAND     PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME\npython3.1 39638 chenzi   10u  IPv4 0x1fced43de6fced48      0t0  TCP *:irdmi (LISTEN)\n', 'status': 'success', 'source': 'local_system'} ---
[计时] 最终回复生成耗时：9.283 秒

Agent: <think>
好的，用户询问8000端口是否被占用。我需要根据工具调用的结果来回答。之前调用了run_simple_system_command执行了lsof -i :8000命令，输出显示有进程在使用该端口。现在需要解析这个输出。

首先，检查命令的输出内容。输出中显示有进程在监听8000端口，具体是python3.1进程，PID是39638。这说明端口确实被占用了。用户可能在尝试启动服务时遇到问题，或者需要确认某个应用是否在运行。

接下来，我需要将结果用自然语言表达出来。需要明确告诉用户端口被占用，并给出进程信息，可能还需要建议如何处理，比如终止进程或更改端口。不过用户的问题只是确认是否被占用，所以先直接回答，再提供进一步帮助的选项。

另外，注意用户可能对命令行不熟悉，所以解释要简洁明了。同时，保持友好，邀请用户提出更多问题。需要确保回答准确且易于理解，避免技术术语过多，除非用户有相关背景。
</think>

当前的8000端口已被占用，正在被进程 `python3.1` (PID: 39638) 监听。这通常意味着某个服务（如开发服务器）正在运行。如果需要释放端口，可以尝试终止该进程或更改服务的端口配置。需要我帮你进一步处理吗？

用户: 当前的ip是
[计时] LLM 思考（工具选择或回复）耗时：14.381 秒

--- LLM 决定调用工具 ---
  工具名称：get_public_ip_address
  调用参数：{}
[计时] 工具 get_public_ip_address 调用耗时：1.936 秒

--- 工具调用结果：{'public_ip': '49.51.200.173', 'source': 'ipify.org'} ---




1. 使用qwen:8b在macs上部署 
 - 不能使用vllm部署.会出现 “RuntimeError("Engine core initialization failed. " RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_0': 1}”的问题 暂时未解决
 - 使用ollama部署： ollama run qwen:8b . 部署成功
 - 使用llama.cpp部署： ./bin/llama-server -m "qwen-model/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf" -c 32768 --port 8080 --jinja -np 4 -cb --no-warmup --threads 8 -ngl 1 部署成功
注意： 
 - 在mac上必须部署gguf(量化)的qwen版本
 - mlx的qwen版本尚未进行测试
2. 工具测试
我们写了32个mcp的tool.都是日常的一些工具.例如获取 当前时间 当前文件夹下的文件 磁盘大小 os命令调用等
2.1 我们测试了 llama.cpp中 think模式的测试
当前只有4个tool的时候
- 工具选择的时间 26秒/5秒/1秒
- llm根据工具的的结果再次进行处理的时间是 0。8秒/1.4秒/4.1秒

有8个tool的时候
- 工具选择的时间 3.6/5.6/5.9
- llm根据工具的的结果再次进行处理的时间是 5.1/5.9/10.5

 有16个tool的时候
- 工具选择的时间 27.2/23/13
- llm根据工具的的结果再次进行处理的时间是 12.3/32/24.7

 有32个tool的时候
- 工具选择的时间 34/50/63
- llm根据工具的的结果再次进行处理的时间是 29/45/32


3. ollama qwen3:8b运行 no think使用工具的情况
- 工具选择的时间 20/14/47
- llm根据工具的的结果再次进行处理的时间是 9.3/1.9/9.3
