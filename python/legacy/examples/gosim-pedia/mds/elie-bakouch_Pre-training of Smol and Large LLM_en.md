
# GOSIM AI Paris 2025

## Host Introduction

*[With upbeat energy]*  
"Ladies and gentlemen, if you've ever wondered how to make a language model both *smol* and mighty, you're in the right place. Please join me in welcoming **Elie Bakouch**, AI Research Engineer at Hugging Face—where he spends his days optimizing language models and his nights probably dreaming in hyperparameters.  

Elie is the brains behind projects like **Open-R1** (an open-source replica of DeepSeek's R1) and the **FineMath** dataset, which proves that even AI needs a good math tutor. With a background in mathematics and computer science from PSL Research University, he's published cutting-edge work on compute-optimal training—because who doesn't love a perfectly scheduled learning rate?  

Today, he'll unpack *'Pre-training of Smol and Large LLMs'*—covering everything from MoE architectures to stability hacks. Think of it as a masterclass in building better models, minus the existential dread of vanishing gradients.  

So, whether you're here for the optimization tricks or just to witness the power of *smol* LLMs, get ready with your questions. Let's give a warm welcome to **Elie Bakouch**!"  

*[Lead applause, transition smoothly]*  

---

## Track Information

### AI Model  
**Shaping the Future with Open-Source AI**  
Unleashing world-class performance in LLMs, multi-modal AI, cutting-edge image and video generation models, and pioneering on-device small LLMs pushing the boundaries of AI efficiency and accessibility.

---

## Event and Session Details

### Conference Overview  
- **Name**: GOSIM AI Paris 2025  
- **Dates**: May 6-7, 2025  
- **Location**: Station F, Paris, France  
- **Theme**: Latest advancements in open-source AI collaboration  
- **Tracks**: AI Model, AI Infra, AI Apps, Embodied AI, AI for Science, Global Open-Source AI Collaboration  

### Session Details  
- **Title**: Pre-training of Smol and Large LLM  
- **Date**: May 7, 2025  
- **Time**: 16:20 - 17:00  
- **Content**: Explaining what's new in pre-training: optimization tricks, MoE, stability hacks, and handling long contexts—everything you need to build better LLMs.  

---

## Speaker Biography

### Professional Overview  
- **Full Name**: Elie Bakouch  
- **Current Position**: AI Research Engineer at Hugging Face  
- **Expertise**: Language model optimization, open-source AI development  
- **Education**: MSc in Mathematics and Computer Science, PSL Research University (2020–2023)  

### Key Contributions  
- Leads training optimization for large/small language models at Hugging Face  
- Principal contributor to **Open-R1** (open-source replication of DeepSeek's R1 model)  
- Developed **FineMath** dataset (34B tokens for math-focused AI training)  

### Notable Publications  
1. **Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations** (2024)  
   - Investigates learning rate scheduling for efficient model training  
   - [arXiv Link](https://arxiv.org/abs/2405.18392)  

2. **FineMath: A High-Quality Dataset for Math-Focused AI Training** (2024)  
   - Introduces curated mathematical corpus for specialized model development  
   - [Project Announcement](https://www.linkedin.com/posts/eliebak_we-just-dropped-finemath-the-best-open-source-activity-7275540160048476161-oqLl)  

### Speaking Engagements  
- **GenAI EUROPE Conference 2024**  
  - Presentation: *Tiny but Mighty: The Power of Smol Language Models*  
  - [Event Page](https://www.aidataanalytics.network/events-generative-ai-europe/speakers/elie-bakouch)  

- **IEEE Spectrum Interview**  
  - Discussion on open-source reasoning model development  
  - [Interview](https://spectrum.ieee.org/deepseek)  

### Technical Leadership  
- Advocates for reproducible, efficient AI through open-source frameworks  
- Regular contributor to Hugging Face's research blog and model repositories  
- Active in community-driven model replication initiatives  

### Digital Presence  
| Platform | Link |  
|----------|------|  
| GitHub | [eliebak](https://github.com/eliebak) |  
| Twitter | [@eliebakouch](https://x.com/eliebakouch?lang=en) |  
| LinkedIn | [Profile](https://fr.linkedin.com/in/eliebak/fr) |  
| Hugging Face | [Research Profile](https://huggingface.co/eliebak) |  

### Recent Media  
- **Video**: *What's New in Pre-Training* (2024)  
  - Technical breakdown of advancements in model pre-training  
  - [Presentation Slides](https://www.linkedin.com/posts/eliebak_gave-a-talk-earlier-today-to-explain-whats-activity-7303432555406483456-fN7g)  

- **Blog**: *Diving into MiniMax01 405B MoE*  
  - Architectural analysis of mixture-of-experts model  
  - [Hugging Face Blog](https://huggingface.co/blog/eliebak/minimax01-deepdive)  

### Academic Background  
- **PSL Research University**  
  - Master's research focused on algorithmic efficiency and mathematical foundations of ML  
  - Thesis work applied to current model optimization techniques  

### Verification Sources  
1. [Official Hugging Face Profile](https://huggingface.co/eliebak)  
2. [Peer-Reviewed Publications](https://scholar.google.com/citations?user=fkysZAkAAAAJ)  
3. [Conference Speaker Verification](https://www.aidataanalytics.network/events-generative-ai-europe)  

---

![Mofa](mofa.png)  
*Content Source: [MOFA](https://github.com/moxin-org/mofa)*  
