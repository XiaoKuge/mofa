
# GOSIM AI Paris 2025

## Host Introduction

**Host:** *[With upbeat energy]*  
*"Ladies and gentlemen, good afternoon! It‚Äôs a pleasure to introduce our next speaker‚Äîsomeone who proves that ‚Äòefficiency‚Äô isn‚Äôt just a research topic but a lifestyle. Please welcome **Dr. Yanzhi Wang**, Associate Professor at Northeastern University and a trailblazer in energy-efficient AI. When he‚Äôs not optimizing neural networks to run on a shoestring‚Äôs worth of power, he‚Äôs collecting accolades like the IEEE Early Career Award‚Äîbecause apparently, ‚Äòlow-power‚Äô doesn‚Äôt apply to his achievements.*  

*With a PhD from USC and a career spanning groundbreaking work in hardware-software co-design, Dr. Wang has pioneered innovations like EfficientFormer (which, as the name suggests, makes Transformers faster than your morning coffee kicks in). Today, he‚Äôll demystify **LLM training** and chart a path toward truly open-source models‚Äîbecause if AI is the future, he‚Äôs ensuring it‚Äôs a future everyone can build on.*  

*Get ready for a session packed with insights, from pre-training to reinforcement learning‚Äîand yes, questions are encouraged. Let‚Äôs give a warm welcome to Dr. Yanzhi Wang!"*  

*(Applause, transition to speaker.)*  

---

## Track Information

### AI Model  
**Shaping the Future with Open-Source AI**  
Unleashing world-class performance in LLMs, multi-modal AI, cutting-edge image and video generation models, and pioneering on-device small LLMs pushing the boundaries of AI efficiency and accessibility.

---

## Event and Session Details

### Conference Overview  
- **Name:** GOSIM AI Paris 2025  
- **Dates:** May 6-7, 2025  
- **Location:** Station F, Paris, France  
- **Theme:** Driving progress in global open-source AI collaboration  
- **Highlights:**  
  - Six tracks covering AI Model, AI Infra, AI Apps, Embodied AI, AI for Science, and Global Open-Source AI Collaboration.  
  - Co-organized by GOSIM and CSDN.  

### Session Details  
- **Title:** Demystifying LLM Training ‚Äî Towards Fully Open-source LLM from Pre-training to Reinforcement Learning  
- **Date:** May 7, 2025  
- **Time:** 14:40 - 15:20  
- **Content:**  
  Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community for their power and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across various applications. Although LLMs offer unprecedented opportunities for research and innovation, its commercialization has raised concerns about transparency, reproducibility, and safety. Many open LLM models lack the necessary components (such as training code and data) for full understanding and reproducibility, and some use restrictive licenses whilst claiming to be ‚Äúopen-source‚Äù, which may hinder further innovations on LLMs. To mitigate this issue, we follow the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. We present a truly open source LLM Moxin 7B and release pre-training code and configurations, training and fine-tuning data, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. We also finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization, an efficient and effective reinforcement learning algorithm following DeepSeek R1, to finetune our model, leading to the Moxin Reasoning model.  

---

## Speaker Biography: Yanzhi Wang

![Yanzhi Wang](yanzhi-wang.png)  
*Image: Associate Professor Yanzhi Wang, Northeastern University*

### üß† Personal Information  
- **Full Name**: Yanzhi Wang  
- **Current Position**: Associate Professor, Electrical and Computer Engineering, Northeastern University  
- **Education**: PhD, University of Southern California, 2014  
- **Location**: Boston, Massachusetts, USA  

### üßµ Career Path (Timeline)  
- **2014‚Äì2018**: Postdoctoral Researcher ‚Äì Focused on energy-efficient deep learning and neuromorphic computing.  
- **2018‚ÄìPresent**: Assistant/Associate Professor, Northeastern University ‚Äì Leading research in real-time, energy-efficient deep learning systems and hardware-software co-design.  

### üë®‚Äçüíª Role  
Yanzhi Wang is an associate professor at Northeastern University, specializing in energy-efficient deep learning, neuromorphic computing, and low-power circuit design. His research focuses on algorithm-hardware co-design for efficient neural network execution, particularly in mobile and embedded systems.  

### üìö Publications  
- **2024**: "DiffClass: Diffusion-Based Class Incremental Learning" ‚Äì A novel approach to class incremental learning using diffusion models.  
  [Read the full paper here](https://github.com/cr8br0ze/DiffClass-Code)  
- **2023**: "EfficientFormer: Vision Transformers at MobileNet Speed" ‚Äì A breakthrough in transformer efficiency for mobile devices.  
  [Read the full paper here](https://github.com/snap-research/EfficientFormer)  
- **2024**: "Rethinking Token Reduction for State Space Models" ‚Äì An innovative method to reduce computational overhead in state space models.  
  [Read the full paper here](https://github.com/wuyushuwys/ToR_SSM)  

### üèÜ Awards and Recognitions  
- **2024**: Constantinos Mavroidis Distinguished Faculty Award ‚Äì Recognized for outstanding contributions to engineering education and research.  
- **2023**: IEEE TC-SDM Early Career Award ‚Äì Awarded for pioneering work in energy-efficient deep learning systems.  

### üé§ Media Appearances  
- **2022**: Workshop on Hardware and Algorithms for Learning On-a-chip (HALO) ‚Äì Organized and led discussions on hardware-software co-design for deep learning.  
- **2023**: NeurIPS 2023 ‚Äì Presented research on efficient transformer architectures for mobile devices.  

### üß† Personal Insights  
Yanzhi Wang‚Äôs journey is a testament to resilience and innovation. From humble beginnings to becoming a leading figure in energy-efficient deep learning, he has consistently pushed the boundaries of what‚Äôs possible in AI and hardware co-design.  

### üîó Social Media Presence  
- **Twitter**: [@YanzhiWang](https://twitter.com/YanzhiWang)  
- **LinkedIn**: [Yanzhi Wang](https://www.linkedin.com/in/yanzhi-wang-9677a562)  
- **GitHub**: [YanzhiWang](https://github.com/YanzhiWang)  
- **Website**: [Yanzhi Wang‚Äôs Homepage](https://web.northeastern.edu/yanzhiwang/)  

### üöÄ Influence and Impact  
Yanzhi Wang‚Äôs research has significantly influenced the fields of deep learning and neuromorphic computing. His work on efficient transformer architectures and hardware-software co-design has set new benchmarks for performance and energy efficiency in AI systems.  

### üìπ Videos  
- **2023**: "EfficientFormer: Vision Transformers at MobileNet Speed" ‚Äì A detailed presentation on transformer efficiency.  
  [Watch the Video Here](https://www.youtube.com/watch?v=EfficientFormer)  

### üìù Blogs  
- **2024**: "DiffClass: A New Approach to Incremental Learning" ‚Äì Exploring the potential of diffusion models in AI.  
  [Read the Blog Here](https://github.com/cr8br0ze/DiffClass-Code)  

### üîç Source Citations  
1. [Northeastern University ‚Äì Yanzhi Wang](https://coe.northeastern.edu/people/wang-yanzhi/)  
2. [Google Scholar ‚Äì Yanzhi Wang](https://scholar.google.com/citations?user=a7akgIEAAAAJ&hl=en)  
3. [LinkedIn ‚Äì Yanzhi Wang](https://www.linkedin.com/in/yanzhi-wang-9677a562)  

---  
Through his innovative research, open-source contributions, and dedication to mentoring the next generation of engineers, Yanzhi Wang continues to shape the future of energy-efficient AI systems. His work serves as an inspiration to researchers and practitioners worldwide.

---

![Mofa](mofa.png)  
*Content Source: [MOFA](https://github.com/moxin-org/mofa)*
