
# GOSIM AI Paris 2025: DeepSpeed ‚Äì Efficient Training Scalability for Deep Learning Models

## üé§ Host Introduction  
*"Ladies and gentlemen, it‚Äôs time to shift gears into something truly *deep*‚Äîpun absolutely intended‚Äîbecause our next speaker is here to talk about scaling AI without scaling your headaches.*  

*Please welcome **Olatunji Ruwase**, a leading expert in deep learning optimization and the brains behind some of the most groundbreaking work in efficient model training. With a Ph.D. from Carnegie Mellon and stints at Microsoft Research and Snowflake, Tunji has spent years making sure AI models don‚Äôt just *think* big but *train* smart. His contributions, like the **ZeRO optimizer**, have literally broken memory barriers‚Äîbecause why settle for billion-parameter models when you can handle *trillions*?*  

*Today, he‚Äôll take us through **DeepSpeed**, the library that‚Äôs making distributed training faster, cheaper, and more accessible‚Äîso you can focus on building the future instead of waiting for your GPU to catch up. Whether you‚Äôre wrestling with compute bottlenecks or communication overhead, this session promises actionable insights to supercharge your workflows.*  

*So get those questions ready‚ÄîTunji‚Äôs the kind of expert who makes complexity look easy. Let‚Äôs give him a warm welcome!"*  

*(Applause, transition to speaker.)*  

---

## üè∑Ô∏è Track Information  
**Track:** *PyTorch Day France*  
A dedicated space for AI practitioners, researchers, and engineers to explore the latest advancements in deep learning with PyTorch. From cutting-edge model development to scalable deployment, this track offers expert insights, hands-on sessions, and discussions shaping the future of AI with PyTorch.  

---

## üìÖ Event & Session Details  
### **Conference Overview**  
- **Name:** GOSIM AI Paris 2025  
- **Dates:** May 6‚Äì7, 2025  
- **Location:** Station F, Paris, France  
- **Theme:** *Global Open-Source AI Collaboration*  
- **Tracks:** AI Model, AI Infra, AI Apps, Embodied AI, PyTorch Day France  

### **Session Spotlight**  
- **Title:** *DeepSpeed ‚Äì Efficient Training Scalability for Deep Learning Models*  
- **Date:** May 7, 2025  
- **Time:** 15:40‚Äì16:00 (CEST)  
- **Content:**  
  Deep Learning (DL) is driving unprecedented progress across AI domains, but scaling demands extreme compute, memory, and communication efficiency. This talk explores **DeepSpeed**, an optimization library that makes distributed model training efficient, effective, and accessible on commodity hardware. Key focus areas include optimizations for compute, communication, and I/O in extreme-scale training.  

![DeepSpeed Optimization](https://via.placeholder.com/800x400?text=DeepSpeed+Training+Scalability)  
*Image: DeepSpeed architecture diagram (credit: Microsoft Research)*  

---

## üéôÔ∏è Speaker Biography  

### Olatunji Ruwase  
**Current Position:** Deep Learning Expert at Snowflake  

### üéì Education  
- **Ph.D. in Computer Science**  
  Carnegie Mellon University  
- **M.S. in Computer Science**  
  Stanford University  
- **B.S. in Computer Science**  
  University of Ibadan, Nigeria  

### üîç Career Highlights  
- **Snowflake (2023):** Advanced large-scale deep learning and optimization techniques.  
- **Microsoft Research (2020s):** Pioneered systems for efficient training/serving of DL models.  
- **Carnegie Mellon University (2010s):** Doctoral research in dynamic binary monitoring under Prof. Todd Mowry.  

### üèÜ Key Contributions  
- **ZeRO Optimizer:** Enabled trillion-parameter model training ([arXiv:1910.02054](https://arxiv.org/abs/1910.02054)).  
- **ZeRO-Infinity:** Overcame GPU memory limits ([arXiv:2104.07857](https://arxiv.org/abs/2104.07857)).  
- **Ms-PoE:** Enhanced long-context processing in LLMs ([arXiv:2403.04797](https://arxiv.org/abs/2403.04797)).  

### üåê Online Presence  
- **Twitter:** [@OlatunjiRu1732](https://twitter.com/OlatunjiRu1732)  
- **LinkedIn:** [Tunji Ruwase](https://www.linkedin.com/in/tunji-ruwase-088952)  
- **GitHub:** [tjruwase](https://github.com/tjruwase)  

![Olatunji Ruwase](http://www.cs.cmu.edu/~oor/photo.jpg)  
*Image: Olatunji Ruwase (credit: Carnegie Mellon University)*  

---

![MOFA](mofa.png)  
*Content Source: [MOFA](https://github.com/moxin-org/mofa)*  
