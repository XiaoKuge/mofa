
# GOSIM AI Paris 2025

## Host Introduction

*"Ladies and gentlemen, thank you for joining us today!*  

*We are thrilled to introduce our next speaker, Dr. Shiwei Liu—a Royal Society Newton International Fellow at the University of Oxford, where he’s busy making deep learning models more efficient, one sparse neuron at a time. Now, if you’ve ever thought, 'Why is my AI so slow?' or 'How do I fit this massive model into my laptop?'—well, Dr. Liu is the person with the answers.*  

*With a Ph.D. earned *Cum Laude* from Eindhoven University and postdoctoral work at UT Austin, he’s become a leading voice in sparse neural networks and efficient LLM training. His work has earned him accolades like the KAUST Rising Star in AI Award and the Royal Society’s prestigious Newton Fellowship—so yes, we’re in *very* good hands today.*  

*In this session, *'The Curse of Depth in Large Language Models,'* Dr. Liu will unravel why deeper layers in LLMs often underperform—and how we can turn that curse into an opportunity for smarter, leaner models. Expect sharp insights, groundbreaking solutions (including his LayerNorm Scaling fix), and maybe even a laugh or two—because who said math can’t be fun?*  

*So, get those questions ready—this is your chance to learn from one of the brightest minds in AI efficiency. Please join me in welcoming Dr. Shiwei Liu!"*  

*(Applause, transition to speaker)*  

---

## Track Information

### **AI Model**  
*Shaping the Future with Open-Source AI*  
Unleashing world-class performance in LLMs, multi-modal AI, cutting-edge image and video generation models, and pioneering on-device small LLMs pushing the boundaries of AI efficiency and accessibility.  

---

## Event and Session Details

### **Conference Overview**  
- **Name:** GOSIM AI Paris 2025  
- **Dates:** May 6-7, 2025  
- **Location:** Station F, Paris, France  
- **Theme:** Driving global open-source AI collaboration across six key tracks.  
- **Highlights:**  
  - Cutting-edge research presentations  
  - Networking with AI leaders  
  - Hands-on workshops and panels  

### **Session Details**  
- **Title:** *The Curse of Depth in Large Language Models*  
- **Date:** May 6, 2025  
- **Time:** 15:40 - 16:20  
- **Content:**  
  Large Language Models (LLMs) have demonstrated impressive achievements. However, recent research has shown that their deeper layers often contribute minimally, with effectiveness diminishing as layer depth increases. This pattern presents significant opportunities for model compression. In the first part of this seminar, we will explore how this phenomenon can be harnessed to improve the efficiency of LLM compression. Despite these opportunities, the underutilization of deeper layers leads to inefficiencies, wasting resources that could be better used to enhance model performance. The second part of the talk will address the root cause of this ineffectiveness in deeper layers and propose a solution. We identify the issue as stemming from the prevalent use of Pre-Layer Normalization (Pre-LN) and introduce LayerNorm Scaling to solve this issue.  

![Dr. Shiwei Liu at the University of Oxford](https://www.maths.ox.ac.uk/sites/default/files/public/favicon.ico)  
*Image Source: [University of Oxford](https://www.maths.ox.ac.uk/people/shiwei.liu)*  

---

## Speaker Biography: Dr. Shiwei Liu

### **Personal Information**  
- **Full Name:** Shiwei Liu  
- **Current Position:** Royal Society Newton International Fellow, Mathematical Institute, University of Oxford  
- **Education:**  
  - **Ph.D.**, Eindhoven University of Technology (2018–2022) – *Cum Laude*  
  - **Master’s Degree**, Harbin Institute of Technology (2015–2017)  
- **Location:** Oxford, United Kingdom  

### **Career Path**  
- **2024–Present:** Royal Society Newton International Fellow, Mathematical Institute, University of Oxford – Leading independent research in advanced deep learning architectures and dynamic sparse training strategies.  
- **2022–2023:** Postdoctoral Fellow, University of Texas at Austin – Contributed to high-impact research in efficient neural network designs and large-scale model pruning.  
- **2018–2022:** Ph.D. Candidate, Eindhoven University of Technology – Developed innovative approaches in sparse deep learning under the mentorship of established researchers.  

### **Research Focus**  
Dr. Shiwei Liu specializes in sparse neural networks, efficient training of large language models (LLMs), and dynamic sparsity. His work leverages low-dimensional structures to enhance model efficiency, robustness, and scalability.  

### **Key Publications**  
- **2021:** *Dynamic Sparse Training: Leveraging Sparsity for Efficient Neural Networks* – A foundational paper on dynamic sparsity in neural networks.  
  [Read the full paper here](https://openreview.net/profile?id=~Shiwei_Liu2)  
- **2023:** *The Curse of Depth in Large Language Models* – Explores the challenges of scaling LLMs.  
  [Read the full paper here](https://openreview.net/profile?id=~Shiwei_Liu2)  
- **2024:** *Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit* – A breakthrough in low-bit training stability.  
  [Read the full paper here](https://openreview.net/profile?id=~Shiwei_Liu2)  

### **Awards and Recognitions**  
- **2024:** Rising Star in AI Award (KAUST) – Recognized for pioneering contributions to sparse deep learning.  
- **2023:** Rising Star Award at the Conference on Parsimony and Learning (CPAL) – For innovative work in efficient neural networks.  
- **2023:** Newton International Fellowship Award (Royal Society & British Academy) – For outstanding research in machine learning.  

### **Media and Outreach**  
- **2024:** *GOSIM AI Paris 2025* – Keynote speaker on efficient training of LLMs.  
- **2024:** *The Role of Sparsity in Deep Learning* – Blog post on the importance of sparsity in neural networks.  
  [Read the Blog Here](https://shiweiliuiiiiiii.github.io/)  

### **Online Presence**  
- **Twitter:** [@Shiwei_Liu66](https://twitter.com/Shiwei_Liu66)  
- **GitHub:** [Shiweiliuiiiiiii](https://github.com/Shiweiliuiiiiiii)  
- **LinkedIn:** [Shiwei Liu](https://uk.linkedin.com/in/shiwei-liu-3b60771ab)  
- **Website:** [Shiwei Liu’s Personal Website](https://shiweiliuiiiiiii.github.io/)  

### **Research Impact**  
Dr. Liu’s work has advanced sparse deep learning, enabling more efficient training and deployment of large neural networks. His contributions influence both academic research and industrial applications, particularly in optimizing LLMs.  

### **Video Content**  
- **2024:** *Efficient Training of Large Language Models* – Lecture on dynamic sparsity and low-bit training.  
  [Watch the Video Here](https://www.maths.ox.ac.uk/people/shiwei.liu)  

### **Source Citations**  
- [University of Oxford Profile](https://www.maths.ox.ac.uk/people/shiwei.liu)  
- [Google Scholar Profile](https://scholar.google.com/citations?user=73IbXtsAAAAJ&hl=en)  
- [LinkedIn Profile](https://uk.linkedin.com/in/shiwei-liu-3b60771ab)  
- [Personal Website](https://shiweiliuiiiiiii.github.io/)  

---

![MOFA](mofa.png)  
*Content Source: [MOFA](https://github.com/moxin-org/mofa)*  
