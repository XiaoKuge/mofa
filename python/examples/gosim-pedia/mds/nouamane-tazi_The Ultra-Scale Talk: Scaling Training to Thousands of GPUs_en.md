
# GOSIM AI Paris 2025: The Ultra-Scale Talk

---

## üé§ Host Introduction  
**Host:**  
*[With upbeat energy]*  
"Ladies and gentlemen, let‚Äôs give a warm welcome to our next speaker‚Äîsomeone who truly lives by the motto *‚ÄòScale it ‚Äôtil you make it.‚Äô* Please join me in welcoming **Nouamane Tazi**, Machine Learning Research Engineer at Hugging Face!  

Nouamane is the brains behind projects like *nanotron*‚Äîa minimalist yet mighty library for training code LLMs‚Äîand key contributions to *StarCoder2*, which speaks 619 programming languages (that‚Äôs 618 more than most of us mastered in lockdown). When he‚Äôs not optimizing trillion-parameter models, he‚Äôs sharing hard-earned wisdom on how to stretch GPU clusters to their limits‚Äîbecause, as we‚Äôll learn, scaling AI isn‚Äôt just about throwing hardware at the problem.  

Today‚Äôs session, **‚ÄòThe Ultra-Scale Talk,‚Äô** is your masterclass in 5D parallelism, performance tuning, and the art of training models so large they‚Äôd make your laptop cry. Whether you‚Äôre battling data constraints or dreaming of thousand-GPU runs, Nouamane‚Äôs insights‚Äîbacked by Hugging Face‚Äôs *Ultra-Scale Playbook*‚Äîwill equip you to think bigger.  

Got questions? Don‚Äôt be shy‚Äîthis is your chance to learn from someone who turns computational impossibilities into *‚ÄòHold my GPU, I‚Äôve got this.‚Äô*  

Without further ado, let‚Äôs dive into the future of scaling‚Äîplease welcome **Nouamane Tazi!**"  

---

## üè∑Ô∏è Track Information  
**Track:** **PyTorch Day France**  
A dedicated space for AI practitioners, researchers, and engineers to explore the latest advancements in deep learning with PyTorch. From cutting-edge model development to scalable deployment, this track offers expert insights, hands-on sessions, and discussions shaping the future of AI with PyTorch.  

---

## üìÖ Event and Session Details  
### **Conference Overview**  
- **Name:** GOSIM AI Paris 2025  
- **Dates:** May 6-7, 2025  
- **Location:** Station F, Paris, France  
- **Theme:** *Global Open-Source AI Collaboration*  
- **Highlights:** Six tracks focusing on AI Model, AI Infra, AI Apps, Embodied AI, AI for Science, and Global Open-Source AI Collaboration.  

### **Session Details**  
- **Title:** *The Ultra-Scale Talk: Scaling Training to Thousands of GPUs*  
- **Date:** May 7, 2025  
- **Time:** 14:40 - 15:00 (CEST)  
- **Content:**  
  Training large language models (LLMs) demands more than just raw compute‚Äîit requires infrastructure, strategy, and a deep understanding of parallelism. What begins as a single-GPU prototype must eventually scale across thousands of devices, each step introducing new complexity.  

  This talk dives into the practicalities of ultra-scale training. We'll explore how 5D parallelism‚Äîspanning data, tensor, pipeline, context, and expert dimensions‚Äîmakes it possible to stretch a single training run across massive GPU clusters. Along the way, we‚Äôll cover performance tuning, communication patterns, and architecture choices that impact throughput and hardware efficiency.  

  A key reference for this session is the *Ultra-Scale Playbook*, which distills best practices and hard-earned lessons from real-world LLM scaling efforts. We‚Äôll walk through highlights of the playbook, tying them into case studies, benchmarks, and hands-on recommendations.  

  Scaling isn‚Äôt just about size‚Äîit‚Äôs about doing more with what you have. This webinar offers a comprehensive look at what it really takes to train state-of-the-art models at scale, designed for engineers, researchers, and practitioners ready to move beyond *‚Äúit fits on one GPU‚Äù* toward infrastructure that powers trillion-parameter models‚Äîefficiently, and at speed.  

![Nouamane Tazi Speaking](https://huggingface.co/nouamane-tazi.jpeg)  
*Image: [Hugging Face](https://huggingface.co/nouamanetazi)*  

---

## üéôÔ∏è Speaker Biography  
### **Nouamane Tazi**  
**Machine Learning Research Engineer at Hugging Face**  

### **Professional Overview**  
Nouamane Tazi is a Machine Learning Research Engineer at Hugging Face, specializing in scaling large language models (LLMs). His work focuses on optimizing training efficiency and transparency, exemplified by projects like *nanotron*‚Äîa minimalist library for training code LLMs. His motto, *‚ÄúScale it ‚Äôtil you make it,‚Äù* underscores his pragmatic approach to overcoming computational constraints in AI.  

### **Key Contributions**  
#### **Research & Publications**  
- **StarCoder2 and The Stack v2** (2024)  
  Next-gen code LLM trained on 619 programming languages.  
  [Paper](https://huggingface.co/papers/2402.19173) | [Blog](https://huggingface.co/blog/starcoder2)  
- **Scaling Data-Constrained Language Models** (2023)  
  Techniques for training LLMs with limited data.  
  [Paper](https://huggingface.co/papers/2305.16264)  
- **MTEB Benchmark** (2023)  
  Multilingual text embedding evaluation across 112 languages.  
  [Paper](https://huggingface.co/papers/mteb)  

#### **Open-Source Impact**  
- Developed *nanotron*, a lightweight framework for efficient LLM training.  
- Contributed to Hugging Face‚Äôs ecosystem for democratizing ML tools.  

### **Speaking Engagements**  
- **The Ultra-Scale Talk** (2024)  
  Keynote on scaling strategies at GPU MODE.  
  [Video](https://twitter.com/Nouamanetazi/status/1896594434185412979)  
- **What‚Äôs New in Pre-Training** (2023)  
  Hugging Face internal talk on LLM advancements.  
  [Slides](https://twitter.com/Nouamanetazi/status/1897719478127149412)  

### **Education & Background**  
- **Universit√© Paris-Saclay**  
  Studies in computer science, physics, and mathematics.  
- Based in Paris, France.  

### **Connect**  
- **GitHub**: [@NouamaneTazi](https://github.com/NouamaneTazi)  
- **Twitter**: [@Nouamanetazi](https://twitter.com/Nouamanetazi)  
- **LinkedIn**: [Profile](https://www.linkedin.com/in/nouamanetazi/)  
- **Website**: [Personal Blog](https://nouamanetazi.github.io)  

![Nouamane Tazi](https://huggingface.co/nouamane-tazi.jpeg)  
*Image: [Hugging Face](https://huggingface.co/nouamanetazi)*  

---

![MOFA](mofa.png)  
*Content Source: [MOFA](https://github.com/moxin-org/mofa)*  
